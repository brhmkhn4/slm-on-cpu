Metadata-Version: 2.1
Name: slm-on-cpu
Version: 0.1.0
Summary: 
Author: Ibrahim Office PC
Author-email: 142000003+brhmkhn4@users.noreply.github.com
Requires-Python: >=3.11,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Dist: ctransformers (>=0.2.27,<0.3.0)
Requires-Dist: langchain (>=0.2.14,<0.3.0)
Requires-Dist: langchain-community (>=0.2.12,<0.3.0)
Requires-Dist: langchain-core (>=0.2.34,<0.3.0)
Requires-Dist: llama-cpp-python (==0.2.89)
Requires-Dist: rich (>=13.7.1,<14.0.0)
Description-Content-Type: text/markdown

#Compilation of llama.cpp with llama-cpp-python using CLBLAST, it is for AMD or Intel GPU.
A simple guide to compile Llama.cpp and llama-cpp-python using CLBlast for older generation AMD GPUs (the ones that don't support ROCm, like RX 5500).

$ cmake -DCMAKE_CXX_COMPILER=C:\MinGW\bin\g++ -DCMAKE_C_COMPILER=\MinGW\bin\gcc pip install llama-cpp-python

# for llama-cpp-python installation run the following command in CMD
```cmd
pip install llama-cpp-python  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu
```

